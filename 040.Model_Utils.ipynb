{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import datetime as datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from azure.datalake.store import core, lib, multithread\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from datetime import date\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_read(path):\n",
    "    with open(path,'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    return df\n",
    "\n",
    "def pickle_write(data,path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formula to calculate r^2\n",
    "\n",
    "def r_square(df,pred,actual):\n",
    "    df['y_pred_diff'] = (df[pred] - df[actual])**2\n",
    "    actual_mean = df[actual].mean()\n",
    "    df['y_mean_diff'] = (df[actual] - actual_mean)**2\n",
    "    r2 =  1 - (sum(df['y_pred_diff']) / sum(df['y_mean_diff']))\n",
    "    df.drop(columns = ['y_pred_diff','y_mean_diff'],inplace = True)\n",
    "    return r2\n",
    "## def a function to log transform columns to normalize it.\n",
    "\n",
    "def logFunc(df, columns):\n",
    "    for i in list(columns):\n",
    "        df[(i+str(1))] = np.where(df[i]==0,0,np.log(df[i]))\n",
    "        df[(i+str(1))] = round(df[(i+str(1))],2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_fsa(b0):\n",
    "    b0['FSA'] = b0['POSTAL_CODE'].str.extract(r'(\\w{3})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_func(df,pred,actual):\n",
    "    df['y_pred_diff'] = (df[pred] - df[actual])**2  \n",
    "    rmse = np.sqrt(df.loc[:,'y_pred_diff'].mean())\n",
    "    df.drop(columns = ['y_pred_diff'],inplace = True) \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formula to calculate mae\n",
    "## mean absolute error\n",
    "def mae_func(df,pred,actual):\n",
    "    df['y_pred_diff'] = abs(df[pred] - df[actual])  \n",
    "    mae = df.loc[:,'y_pred_diff'].mean()\n",
    "    df.drop(columns = ['y_pred_diff'],inplace = True) \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_func(df,pred,actual):\n",
    "    df['y_pred_diff_percent'] = abs( (df[actual] - df[pred])/df[actual] )\n",
    "    mape = df.loc[:,'y_pred_diff_percent'].mean()\n",
    "    df.drop(columns = ['y_pred_diff_percent'],inplace = True)\n",
    "\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maape_func(df,pred,actual):\n",
    "    df['y_pred_diff_percent'] = np.arctan(abs( (df[actual] - df[pred])/df[actual] ))\n",
    "    maape = df.loc[:,'y_pred_diff_percent'].mean()\n",
    "    df.drop(columns = ['y_pred_diff_percent'],inplace = True)\n",
    "\n",
    "    return maape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vwmape_func(df,pred,actual):\n",
    "    df['y_pred_diff_percent'] = abs(df[actual] - df[pred])\n",
    "    vwmape = df['y_pred_diff_percent'].sum() / df[actual].sum()\n",
    "    df.drop(columns = ['y_pred_diff_percent'],inplace = True)\n",
    "\n",
    "    return vwmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Light_GBM(x_train,y_train,x_test,y_test,cat_vars,early_stopping_rounds,num_boost_round,verbose_eval):\n",
    "    print()\n",
    "    print(format('start train light gbm model with basic setup','*^82'))\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(x_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "    params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 1000,\n",
    "    'learning_rate': 0.3,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "    \n",
    "    evals_result = {}  # to record eval results for plotting\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=num_boost_round,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                categorical_feature=cat_vars,\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=verbose_eval)\n",
    "    \n",
    "    print(format('basic model training is done','*^82')) \n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## by using each feature's threshold, to generate the top features based on selection criteria\n",
    "\n",
    "def LightGBM_features_refined(model,x_train,y_train,x_test,y_test,target):\n",
    "    print()\n",
    "    print(format('start feature refined selection based on feature ranks','*^82'))\n",
    "    features = pd.DataFrame(columns = ['feature'],data = model.feature_importance(importance_type='gain'))\n",
    "    features['featureName'] = model.feature_name()\n",
    "    features.sort_values('feature',ascending = True,inplace = True)\n",
    "    features = features.reset_index()\n",
    "    \n",
    "    train = pd.concat([x_train,y_train],axis = 1)\n",
    "# select features using threshold\n",
    "    test  = pd.concat([x_test,y_test],axis = 1)\n",
    "    print('running accumulative features with corresponding metrix:')\n",
    "    for i in range(0, len(features.featureName)):\n",
    "        select_X_train = train[features.featureName[i:len(features.featureName)]]\n",
    "        y_train1 = train[target]\n",
    "        selection_model = lgb.LGBMRegressor()\n",
    "        selection_model.fit(select_X_train, y_train1)\n",
    "    \t# eval model\n",
    "        select_X_test = test[features.featureName[i:len(features.featureName)]]\n",
    "        select_X_test.reset_index(inplace = True,drop = True)\n",
    "        \n",
    "        y_test1 = test[target]        \n",
    "        y_test1.reset_index(inplace = True,drop = True)\n",
    "        \n",
    "        y_pred1 = selection_model.predict(select_X_test)\n",
    "    \t\n",
    "        y_pred_data1 = pd.DataFrame(columns = ['Pred'],data = y_pred1)\n",
    "       \n",
    "        test11 = pd.concat([select_X_test,y_pred_data1,y_test1],axis = 1)  \n",
    "        \n",
    "        \n",
    "        #test11['Pred'] = np.where(test11['Pred'] <0 ,0,test11['Pred'])\n",
    "        test11['Pred'] = round(test11['Pred'],2)\n",
    "        \n",
    "        r2_v1 = r_square(test11,'Pred',target)\n",
    "        mae = mean_absolute_error(y_test1, y_pred1)\n",
    "        accuruacy  = mape_func(test11,'Pred',target)\n",
    "        accuruacy_adj = vwmape_func(test11,'Pred',target)\n",
    "        \n",
    "        print(\"---%s, Thresh=%d, n=%d, R2: %.2f, mae: %.2f, accuruacy_adj: %.3f---\" % (\n",
    "                features['featureName'][i], features['feature'][i], select_X_train.shape[1], r2_v1,mae ,1-accuruacy_adj))\n",
    "    print('please select your refined features based on this analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParameterOptimizer(model,parameters,x_train,y_train,cat_vars,early_stopping_rounds,eval_set): \n",
    "    print()\n",
    "    print(format('To find optimal parameters for lightGBM using GridSearchCV for Regression','*^82'))    \n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2,n_jobs = -1)\n",
    "    grid.fit(x_train,y_train,categorical_feature=cat_vars,eval_metric = 'rmse',\n",
    "             early_stopping_rounds =early_stopping_rounds,eval_set =eval_set,verbose = 0)    \n",
    "    \n",
    "    # Results from Grid Search\n",
    "    print(\"\\n========================================================\")\n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"========================================================\")    \n",
    "    \n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "          grid.best_estimator_)\n",
    "    \n",
    "    \n",
    "    print(\"\\n The best score across ALL searched params:\\n\",\n",
    "          grid.best_score_)\n",
    "    \n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "          grid.best_params_)\n",
    "    \n",
    "    print(\"\\n ========================================================\")\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate prediction on holdout data\n",
    "\n",
    "def predData(x_test,y_test,bestModel,target,log= True):\n",
    "\n",
    "    print(format(\"Start to predict on test/validation dataset\",'*^82'))\n",
    "    best_model= bestModel.best_estimator_\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "    ## create test dataset with prediction\n",
    "    y_pred_data = pd.DataFrame(columns = ['Pred'],data = y_pred)\n",
    "    \n",
    "    #train = pd.concat([x_train,y_train],axis = 1)\n",
    "    test = pd.concat([x_test,y_test],axis = 1)\n",
    "    test.reset_index(inplace = True)\n",
    "    test1 = pd.concat([test,y_pred_data],axis = 1)\n",
    "    \n",
    "\n",
    "    test1['Pred'] = round(test1['Pred'],2)\n",
    "    \n",
    "    if log == True:\n",
    "        test1['Pred'] = test1['Pred']\n",
    "    if log == False:\n",
    "        test1['Pred'] = np.where(test1['Pred'] <= 0, 0, test1['Pred'])\n",
    "\n",
    "    print('Calculate r2 for test/validation data')\n",
    "    r2_v1 = r_square(test1,'Pred',target)\n",
    "    \n",
    "    print(\"r2 for test data: %.2f\" % (r2_v1))\n",
    "    # eval\n",
    "    print('The rmse of prediction is:', mean_squared_error(y_test, test1['Pred']) ** 0.5)\n",
    "    return test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function for plotting graph\n",
    "\n",
    "def modelPlot(bestModel):\n",
    "    best_model= bestModel.best_estimator_\n",
    "    #print('Plotting metrics recorded during training...')\n",
    "    #ax = lgb.plot_metric(evals_result, metric='l1')\n",
    "    #plt.show()\n",
    "    \n",
    "    print('Plotting feature importances...')\n",
    "    ax = lgb.plot_importance(best_model, max_num_features=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot by comparing prediction vs actuals\n",
    "def predEval(test,prediction,actual):\n",
    "     ## residule plot\n",
    "    x = test[actual]\n",
    "    y = test[prediction]\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    # Plot the data\n",
    "    data_line = ax.plot(y,x,'or',label='_nolegend_')\n",
    "    \n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(x,p(x),\"r--\",color = 'black',label = 'Actual_trend')\n",
    "    plt.plot(x,x,\"r--\",color = 'green',label = 'Ideal_trend')\n",
    "    \n",
    "    \n",
    "    # add a title\n",
    "    ax.set_title(\"Prediction vs Actuals\")\n",
    "    \n",
    "    # add label\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Actuals')\n",
    "    \n",
    "    # Make a legend\n",
    "    legend = ax.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot by comparing prediction vs actuals\n",
    "def predEval(test,prediction,actual):\n",
    "     ## residule plot\n",
    "    x = test[actual]\n",
    "    y = test[prediction]\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    # Plot the data\n",
    "    data_line = ax.plot(y,x,'or',label='_nolegend_')\n",
    "    \n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(x,p(x),\"r--\",color = 'black',label = 'Actual_trend')\n",
    "    plt.plot(x,x,\"r--\",color = 'green',label = 'Ideal_trend')\n",
    "    \n",
    "    \n",
    "    # add a title\n",
    "    ax.set_title(\"Prediction vs Actuals\")\n",
    "    \n",
    "    # add label\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Actuals')\n",
    "    \n",
    "    # Make a legend\n",
    "    legend = ax.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residulPlot1(test,prediction,actual,bins):\n",
    "    ## residule plot\n",
    "    x = test[actual]\n",
    "    difference = test[prediction] - x\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    # Plot the data\n",
    "    data_line = ax.hist(difference, bins = bins, color = 'blue', edgecolor = 'black')\n",
    "   \n",
    "    # add a title\n",
    "    ax.set_title(\"Residual Histogram\")\n",
    "    \n",
    "    # add label\n",
    "    ax.set_xlabel('Residual')\n",
    "    ax.set_ylabel('Freq')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This a function that will be used by a hypotetic user.\n",
    "\n",
    "def conf_int_user(df, target, level={'FSA': None, 'FULL_TERM_CLASS':None , 'DRIVER_SEX_CODE_D1':None}, pred = None, \n",
    "                  nb_samples=150, alpha = 0.1, adjust_int = True):\n",
    "    \"\"\"This function takes optional levels and a optional prediction (which normally will not be provided by the \n",
    "    user). It returne the average of the predictions at that levels, the confidence interval around it, \n",
    "    and the mean of the truth at those level (this last one will be useful for some statistical \n",
    "    studies, but will not be showed to a normal user).\n",
    "    arguments:\n",
    "    df: is the holdout data set having all features, truth and predictions, \n",
    "    alpha: determines the confidence level = 100*(1-2*alpha),\n",
    "    adjust_int= True: will adjust the confidence interval to contains 'pred'. However, if 'pred' is not provided it is \n",
    "    unlikely that the provided prediction falls outside the confidence interval. \n",
    "    outputs: \n",
    "    pred: is the predicted values given the level. It is average of the predictions, using the data \n",
    "    set that is used for computing the confidence interval,\n",
    "    q_mn: lower bound of the confidence interval,\n",
    "    q_mx: upper bound of the confidence interval,\n",
    "    was_out: True if the predicted values for 'pred' was outside of the confidence interval befor adjusting it,\n",
    "    avg_truth: the average of the actual values in the dataset used to produce confidence interval. \n",
    "    \n",
    "    \"\"\"\n",
    "    df1 = df.copy()\n",
    "    if level is not None:\n",
    "        for k , v in level.items():\n",
    "            if v is not None: \n",
    "                if df1[df1[k] == v].shape[0] >= nb_samples:\n",
    "                    df1 =df1[df1[k] ==v]\n",
    "    if pred is None: \n",
    "        pred = df1.Pred.mean()\n",
    "    if df1.shape[0] > nb_samples:\n",
    "        k=1\n",
    "        lent =0\n",
    "        while lent < nb_samples: \n",
    "            df2= df1[(df1.Pred < pred+5*k) & (df1.Pred > pred-5*k)]\n",
    "            lent = df2.shape[0]\n",
    "            k+=1  \n",
    "\n",
    "    avg_truth = df2[target].mean()\n",
    "    res = df2[target] - df2.Pred\n",
    "    q_mn , q_mx = res.quantile(alpha, interpolation='higher') , res.quantile(1-alpha, interpolation='lower')\n",
    "    was_out = q_mn*q_mx > 0\n",
    "    q_mn = pred+q_mn\n",
    "    q_mx = pred+q_mx\n",
    "    if adjust_int:\n",
    "        q_mn = np.amin([q_mn, pred])\n",
    "        q_mx = np.amax([q_mx, pred])\n",
    "    return (q_mn, pred, q_mx, was_out, avg_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(silver_df, datalake=False):\n",
    "    features = ['AGE_D1',\n",
    "                'POSTAL_CODE',\n",
    "                'COMPANY_CODE',\n",
    "                'COVERAGE_CODE_AUTOMOBILE',\n",
    "                'DISTANCE_DRIVEN_ANNUALLY',\n",
    "                'VEHICLE_CODE1',\n",
    "                'DRIVING_AGE',\n",
    "                'VEHICLE_AGE',\n",
    "                'MULTICAR_DISCOUNT_INDICATOR',\n",
    "                'DRIVERS_NUM',\n",
    "                'FULL_TERM_PREMIUM_COVERAGE',\n",
    "                'DRIVER_SEX_CODE_D1',\n",
    "                'AUTO_CLAIMS',\n",
    "                'CONVICTION_NUM',\n",
    "                'DRIVER_MARITAL_STATUS_CODE_D1',\n",
    "                'VIN_SERIAL_NUMBER',\n",
    "                'COVERAGE_CODE_AUTOMOBILE_CLASS'\n",
    "                ]\n",
    "    train_df = silver_df[features]\n",
    "    num_cols =['AGE_D1',\n",
    "               'DEDUCTIBLE_AMOUNT',\n",
    "               'DISTANCE_DRIVEN_ANNUALLY',\n",
    "               'LIMIT_1',\n",
    "               'VEHICLE_RATE_GROUP',\n",
    "               'DRIVING_AGE',\n",
    "               'VEHICLE_AGE',\n",
    "               'DRIVERS_NUM',\n",
    "               'AUTO_CLAIMS',\n",
    "               'CONVICTION_NUM',\n",
    "               'FULL_TERM_PREMIUM_COVERAGE'\n",
    "                ]\n",
    "\n",
    "    target = ['FULL_TERM_PREMIUM_COVERAGE']\n",
    "\n",
    "    cat_cols = ['COMPANY_CODE',\n",
    "                'COVERAGE_CODE_AUTOMOBILE',\n",
    "                'VEHICLE_CODE1',\n",
    "                'MULTICAR_DISCOUNT_INDICATOR',\n",
    "                'POSTAL_CODE',\n",
    "                'DRIVER_SEX_CODE_D1',\n",
    "                'DRIVER_MARITAL_STATUS_CODE_D1',\n",
    "                'NEW_CITY_NAME',\n",
    "                'VIN_SERIAL_NUMBER',\n",
    "                'PROVINCE_STATE_ABBREVIATION'\n",
    "                ]\n",
    "\n",
    "    for col in num_cols:\n",
    "        train_df[col] =pd.to_numeric(train_df[col],errors='coerce')\n",
    "    train_df = train_df[train_df['DRIVING_AGE'].notnull()]\n",
    "    train_df = train_df[train_df['VEHICLE_CODE1'].notnull()]\n",
    "    train_df = train_df[train_df['VIN_SERIAL_NUMBER'].notnull()]\n",
    "    train_df = train_df[train_df['DISTANCE_DRIVEN_ANNUALLY'].notnull()]\n",
    "    train_df = train_df[train_df['CONVICTION_NUM'].notnull()]\n",
    "    train_df = train_df[train_df['DRIVER_SEX_CODE_D1'].notnull()]\n",
    "    train_df = train_df[train_df['DRIVER_MARITAL_STATUS_CODE_D1'].notnull()]\n",
    "    train_df = train_df[train_df['FSA'].notnull()]\n",
    "    train_df[['LIMIT_1','DEDUCTIBLE_AMOUNT','FULL_TERM_PREMIUM_COVERAGE']] = train_df[['LIMIT_1','DEDUCTIBLE_AMOUNT','FULL_TERM_PREMIUM_COVERAGE']].fillna(0)\n",
    "    train_df['POSTAL_CODE'] = train_df['POSTAL_CODE'].str.extract(r'([A-Z]\\d\\w\\d\\w\\d)',expand=False)\n",
    "    train_df = train_df[train_df['POSTAL_CODE'].notnull()]\n",
    "    train_df['MULTICAR_DISCOUNT_INDICATOR'] = train_df['MULTICAR_DISCOUNT_INDICATOR'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    #set range for age\n",
    "    train_df = train_df[(train_df['AGE_D1']>=15)&(train_df['AGE_D1']<=95)]\n",
    "    #set range for full_term_premium\n",
    "    train_df= train_df[(train_df['FULL_TERM_PREMIUM_COVERAGE']>-200)&(train_df['FULL_TERM_PREMIUM_COVERAGE']<1000)]\n",
    "    #set range for DRIVING_AGE\n",
    "    train_df = train_df[(train_df['DRIVING_AGE']>=0)&(train_df['DRIVING_AGE']<=85)]\n",
    "    \n",
    "    input_cols = list(set(train_df.columns) - set(target))\n",
    "    \n",
    "    x_train = train_df[input_cols]\n",
    "    y_train = train_df[target]\n",
    "    \n",
    "    if datalake ==True:\n",
    "        with adl.open('/LeadGen/Raw/Silver/AUTO/Train_data/x_train_'+today+'.pkl', 'wb') as f:\n",
    "            pickle.dump(x_train,f,pickle.HIGHEST_PROTOCOL)\n",
    "        print('file has been created')\n",
    "        with adl.open('/LeadGen/Raw/Silver/AUTO/Train_data/y_train_'+today+'.pkl', 'wb') as f:\n",
    "            pickle.dump(y_train,f,pickle.HIGHEST_PROTOCOL)\n",
    "        print('file has been created')\n",
    "    else:\n",
    "        \n",
    "        return x_train,y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
